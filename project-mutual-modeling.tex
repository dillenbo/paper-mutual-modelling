\documentclass{report}

\usepackage{tikz}
\usetikzlibrary{shapes}

\usepackage{subcaption}

\newcommand{\ie}{{\textit{i.e.\ }}}
\newcommand{\cf}{{\textit{cf\ }}}
\newcommand{\eg}{{\textit{e.g.\ }}}
\newcommand{\al}{{\textit{et al.\ }}}

\newcommand{\M}[3]{{\mathcal{M}(#1, #2, #3)}}
\newcommand{\model}[3]{{$\mathcal{M}(#1, #2, #3)$}}
\newcommand{\Model}[3]{{$\mathcal{M}^{\circ}(#1, #2, #3)$}}

\newcommand{\concept}[1]{{\small \texttt{#1}}}
\newcommand{\stmt}[1]{{\footnotesize \tt $\langle$ #1\relax$\rangle$}}



\begin{document}

\title{Mutual Modeling in Human-Robot Interaction - Scientific part}

\chapter{Scientific Part}
\section{Summary}

Human-robot interaction is a challenge for artificial intelligence. This field
indeed lays at the crossroad of several other domains of AI and requires to
tackle them in a holistic manner: Modeling humans and human cognition;
acquiring, representing, manipulating in a tractable way abstract knowledge;
reasoning on this knowledge to make decisions; and eventually instantiating
those decisions into physical actions in coordination with humans. Many AI
techniques are invited, from visual processing to symbolic reasoning, from task
planning to \emph{theory of mind} building, from reactive control to action
recognition and learning.

\paragraph{Theory of Mind}

Theory of Mind (originally defined in~\cite{premack1978does}) is the cognitive
ability that a subject possesses to represent the mental state of another
agent, possibly including knowledge that contradicts the subject's own model: for
example, a book can be at the same time \emph{visible} for myself, and \emph{not
visible} for you.

Children develop this skill, which is essential to understand others'
perspectives during interactions, around the age of three. It supposes the
ability to build, store and retrieve separate models of the knowledge of the
interactors.

One classical application of this cognitive skill is the so-called
\emph{False-Belief} experiment (also known as the \emph{Sally and Ann}
experiment)~\cite{Leslie2000}: a child is asked to watch a scene where two
people, A and B, manipulate objects. Then A leaves and B hides away one
object. When A comes back, we ask the child ``where do you think A will
look for the object?''. Before acquiring a theory of mind, children are not
able to separate their own (true) model of the world (where they know that
the object was hidden) from the model of A, which contains \emph{false
beliefs} on the world (A still thinks the object is at its original
position since he did not see B hiding it).


\paragraph{Mutual Modelling}

In order to build a shared understanding, do partners have to build a
representation of each other's knowledge? We refer to \emph{mutual modeling} (MM) as
the process of inferring one's partner mental states. Any claim that students
carry out a detailed monitoring of their peers would be as incorrect as any
claim that they do not maintain any representation at all. If mutual modeling
had to be permanently detailed and accurate, subjects would obviously face a
huge cognitive load. Conversely, peers could not collaborate without some
minimal amount of mutual modeling. For instance, A cannot disagree with B
without knowing that B has a different opinion. The mutual model can be implicit
(A is not aware of what he knows about B), by-default (I believe that B beliefs
what I believe unless contrary evidence), opportunistic (A does not model B
unless the conversation requires it), global (A infers B's beliefs based on
categories such as age, culture or profession) and, of course, it can be
incorrect… but it can not remain empty. Dialogues include many instances of
utterances such as "I thought he would do that" (first level of MM) or even "He
thought I would do that but I intended something else." (second level of MM). 

The content of mutual models ranges from 'dispositional' versus 'situational'
aspects. The 'dispositional' aspects refer to A's representation of B's long
term knowledge, skills or traits. It is thus closely related to the notion of
transactive memory~\cite{wegner1987transactive, moreland1999transactive}.
'Situational' aspects refer to A's representation of B's knowledge, behavior or
intentions specifically activated in the situation in which A and B are
collaborating, some of them being valid for 2 seconds, other ones for 2 hours.
Here are examples of fragments that constitute A's model of B regarding to
aspects X, noted $Model(A,B,X)$:

\begin{itemize}

    \item $Model(A,B, knowledge)$: What does A know about B's knowledge with
        respect to the task at hand or, inversely, about B's knowledge gaps?
        When can A consider B's statements as reliable? 

    \item $Model(A,B, skills)$: What does A know about B's skills with respect to
        the task at hand? May A expect B to perform well in a specific subtask?
        The effectiveness of division of labor depends on the quality of this
        mutual model. 

    \item $Model(A,B, goals)$: What does A know about B's intentions with respect
        to the project, including B' motivation and commitment? Can A trust B
        when B promises to deliver? 

    \item $Model(A,B, task)$: What does A know about B's representation of the
        situation and the task: does A knows whether B has the same
        understanding of the problem at stake? 

    \item $Model(A,B, plans)$: What does A know about B's strategy. Does A
        understand why B did what he did? Is A able to anticipate what B will do
        next? 

    \item $Model(A,B, "urgent")$: What does A about know B's understanding of A's
        last utterance: does ‘urgent’ means now, ASAP and ‘not too lat’ ?

    \item We could continue the list of what is X is \model{A}{B}{X}: beliefs,
        emotions, history, status, …

\end{itemize}

We have different levels of MM. If A states "B thinks I am god in maths", we
are the second level of MM:  $Model(A, B, Model(B,A, knowledge="good in
math"))$. There is an infinite regress of nested models: If A says "B knows that
I don't expect him to solve this statistics problem" corresponds to $Model(A, B,
Model (B, A (Model (A,B, statistic-skills)))$.

A partner model is probably not a "box", \ie not a monolithical representation
but rather a mosaic of information fragments about the partner, with various
grain sizes and various life cycles. This mosaic is elaborated through various
mechanisms, first for building an initial model of the partner and then for
updating this model.  As two students meet for the first time, mutual models are
initialized by the assumptions they make upon each other from cues such as
his/her membership to large categories (age, culture, profession, ...) include
stereotypes (sportsmen, junkie, business women, Swiss,...) as well as physical
appearance. Scholars studied how initial modeling impacts communication. In
their experiments on initial MM, Slugoski, Lalljee, Lamb \&
Ginsburg~\cite{slugoski1993attribution} pretended to their subjects that their
(fake) partner had or had not received the same information. They observed that
the subjects adapted their dialogue by focusing the explanation on the items
that (s)he was supposed to ignore. Brennan (1991) showed that the subjects used
different initial strategies in forming queries depending on who they were told
their partner was.  

Initial common grounds are also initiated by co-presence: they include events to
which A and B attended together~\cite{clark2002definite} in the physical space
or in our cultural space (\eg ‘09-11’). Co-presence means that we can refer to
a shared objects and event,-s it does of course not imply that we give them the
same meaning. Namely, a shared screen does not mean a shared
understanding~\cite{dillenbourg2006sharing}.

After initialization, mutual models are updated along the collaborative work
through verbal and non-verbal interactions. A default inference rule is that "my
partner agrees with me unless he disagrees", which reject the critiques that
mutual modeling generates an unbearable cognitive load. This default rule is
superseded by the several mechanisms for monitoring and repairing one's partner
understanding: acknowledgement, continuous attention, relevance of next turns,
facial expressions including gaze signals, etc.

Mutual modeling does not occur in a vacuum but it is highly contextualized. REF
reviews the features of the collaborative situation, namely the media
(cotemporality… ), may facilitate or hamper mutual modeling. Hutchins (SILENCE)
reported a study in which a short silence between two pilots was perfectly
interpreted because it occurred in a highly constrained communication context.
Some environments are more productive than others in helping peers to detect
their misunderstandings. Roschelle~\cite{roschelle1995construction} reformulate
the design of CSCL interfaces as providing ways for peer to detect and repairs
their misunderstanding. In CSCW, researchers have explored various so-called
"awareness tools", \ie functionalities that inform A is about B's actions that A
could not directly perceived because B was working on a different subset of the
virtual space. Different awareness tools will be addressed in this contribution
since they allowed us to manipulate experimentally the MM activity. 

\section{Research Plan}

\subsection{State of the Art}

\paragraph{Building a model of a human during the interaction} Perspective
Taking is a human ability which allows one to put him/herself in another
person's point of view. Studied in psychology
literature~\cite{Flavell1992,Tversky1999}, this ability is crucial when
interacting with people by allowing one to reason on others' understanding of
the world in terms of visual perception, spatial descriptions, affordances and
beliefs, etc.  In the last years these notions have been gradually employed in
HRI.~\cite{Breazeal2006} presents a learning algorithm that takes into account
information about a teacher's visual perspective in order to learn a task.
~\cite{Johnson2005} apply visual perspective taking for action recognition
between two robots.~\cite{Trafton2005} use both visual and spatial perspective
taking to find out the referent indicated by a human partner.

The cognitive model that the robot builds for the agent it interacts with is
still simple and mostly focused on geometric features (who sees what? What are
our relative positions? etc.). Extending this knowledge with more subtle
perceptions (emotional state for instance) remains to be done.

\paragraph{Joint action}
Several theories dealing with collaboration~\cite{Cohen1991,Grosz1996,Clark1996}
emphasize that collaborative tasks have specific requirements compared to
individual ones, \eg, since the robot and the person share a common goal, they
have to agree on the manner to realize it, they must show their commitment to
the goal during execution, etc. Several robotic systems have already been built
based on these theories~\cite{Rich1997,Sidner2005,Breazeal2003} and they all
have shown benefits of this approach. They have also shown how difficult it is
to manage turn-taking between communication partners and to interleave task
realization and communication in a generic way. Finally, today only few
systems~\cite{Fong2006,Breazeal2003,Sisbot2008} take humans into account at all
levels.

\subsection{Related research of the requirers}

Our knowledge base implements such a mechanism: when the robots infers a new
agent has been introduced in the knowledge base, it initializes a new,
independent, ontology for this agent. All the ontologies that are created share
the same common-sense knowledge, but rely on each agent's perspective for the
actual instantiation: the robot (geometrically) computes that the book is in
its own field of view, but not in the human one. The robot knowledge contains
the fact \stmt{book isVisible true} while the human model contains \stmt{book
isVisible false}.

Using separate knowledge models in the knowledge base, we have been able to
replicate the \emph{Sally and Ann} false-belief experiment with our
robots~\cite{warnier2012when}.

In our implementation, perspective taking is tightly connected to the symbolic
knowledge models, and since our knowledge base allows for storage of one
knowledge model per agent, we have been able to endow the robot with a simple
theory of mind: we explicitly model
what the robot knows about its partners in a symbolic way. This knowledge is
then re-used in different places, to correctly interpret what the human says,
or to plan tasks that are actually doable for the human.




\subsection{Detailed research plan}

\subsubsection{Research questions}


The core research question we want to address in this project is
$(\mathcal{Q}_1)$ how mutual modeling impacts the quality of the human-robot
interaction?  \emph{Quality}, in our context, is measured as the performance of
the human-robot pair in selected joint tasks.

To actually instigate a mutual modeling situation, three other research
questions must be addressed. $(\mathcal{Q}_2)$ how can the robot acquire and
represent a mental model of its partners? $(\mathcal{Q}_3)$ conversely, how do
the human partners model the robot and what are the robot's design/behavioural
factors that influences it? $(\mathcal{Q}_4)$ once the robot detects
misunderstandings, what should be the interaction strategy?
\subsubsection{A theoretical model of mutual modeling}


To report experiments on MM, we use a simple notational system: ``A knows that B
knows X", noted \model{A}{B}{X}. This is a simple but hopefully useful reduction to
reason on mutual modeling. This notation does not mean A has an explicit,
monolithic representation of B: it must be understood as an abstraction referring to complex
socio-cognitive processes.

We refer to the degree of accuracy of the mutual model as
\Model{A}{B}{X}. To estimate the MM effort, we need 3 variables.

\begin{enumerate}

    \item Tasks vary a lot with respect to how much they require.  The grounding
        criterion – denoted $\mathcal{M}^{\circ}_{min}$  - refers to how
        important it is to mutually share a piece of information X to succeed
        the task T.. It can be computed as the probability to succeed T despite
        the fact X is not grounded. In the next studies, we estimate
        $\mathcal{M}^{\circ}_{min}(A,B,X)$ as the correlation between
        \Model{A}{B}{X} and task performance. 

    \item Before any specific grounding act, there is rarely a null probability
        that X is mutually understood by A and B: because X is part of A's and
        B's cultures, because it is manifest to co-present subjects or simply
        because there is not much space for misunderstanding or disagreement
        about X. We could not simply collaborate without a high level of initial
        grounds. We note the theoretical accuracy of initial grounds:
        $\mathcal{M}^{\circ}_{t1}(A,B,X)$.

    \item The cost of grounding X refers to the physical and cognitive effort
        necessary to perform a grounding act α: a verbal repair (\eg
        rephrasing), a deictic gesture, a physical move to adopt one partner's
        viewpoint, etc. This cost varies according to media features ( Clark \&
        Brennan (REF)). 

\end{enumerate}

Based on these 3 parameters, the probability of making an action $\alpha_{t1}$ about
content X at time $t1$ during task T for increasing \Model{A}{B}{X}
at time $t2$ is the ratio between how much it is needed  ((2)-(1)) and how much it
costs (3)(Traum \& Dillenbourg, 1996):

$p(\alpha_{t1}(X,T)   M^{\circ}_{t2}(A,B,X)) ≈ (M^{\circ}_{min}(A,B,X,T) -
M^{\circ}_{t1}(A,B,X))/cost (\alpha)$

This is qualitative summary more that a real equation since several parameters
are hard to quantify (\eg the cost an communication acts depends upon the user
as well) but it clarifies the parameters of the experiments we report hereafter.

\begin{itemize}
    \item \Model{A}{B}{X}: Our experiments address different contents that can be
        represented in mutual models:

        \begin{enumerate}
            \item \model{A}{B}{emotion}: how accurately A perceives B's emotional state
                (study 3); 

            \item \model{A}{B}{actions} is about how well A guesses what action B has
                done (study 2) or will do next  (study 1)

            \item \model{A}{B}{knowledge}: how accurately A estimates B's knowledge
                with respect to the material they learn together (study 4 and 5)

        \end{enumerate}

    \item \Model{A,B}{X}{T} Our studies concern various collaborative tasks:
        argumentation (study 3), games (study 1 and 2) and concept mapping
        (study 4 and 5). By varying the tasks, we do actually vary the grounding
        criterion but they all require a "reasonably high" grounding criterion.
        All tasks are about building a solution or a representation together; we
        did not address everyday conversations. 

    \item $\mathcal{M}^{\circ}_{t1}(A,B,X)$ Along the same reasoning, the initial degree
        of common grounds should be rather low (and hence the difference between
        initial and required degrees rather high) in order to make mutual
        modelling effort more observable. Studies 1, 4 and 5 have been conducted
        with teams of students who did not know each other. They came
        nonetheless from the same university: they hence had some general common
        grounds.  For studies 2 and 3, students knew each other before for
        reasons explained later on. In study 5, we manipulated the initial MM by
        using a JIGSAW script.

    \item $cost(\alpha)$ In all studies but study 4, the cost of  grounding is
        an independent variable. Study 3 uses media richness as independent
        variable, with the hypothesis that modeling emotions is "cheaper" with a
        richer medium, \ie  when peers can see each other.  Studies 1,2 and 4
        use awareness tools which, in principle, reduce the cost of MM, but do
        not eliminate all costs: if the tool provide A with information about
        what B does/knows, this additional information may actually increase
        cognitive load. Awareness tools constitute as a kind of MM prosthesis,
        and, like any prosthesis, they many augment MM (by facilitating it or
        even scaffolding it) or inhibit it (by making it useless).

\end{itemize}


Measuring \Model{A}{B}{X} is methodologically difficult. We
discriminate two steps, first to capture \model{A}{B}{X} and then to
estimate \Model{A}{B}{X}. 

\begin{itemize}
    \item Capturing M (A,B,X) The simplest method is to ask A what (s)he
        believes about what B knows, feels, intends to do, etc. This raises
        obvious methodological concerns since such a question triggers a
        modeling process beyond what it would 'naturally' be. To avoid this
        bias, one can estimate mutual modeling after task completion. Then, the
        obvious drawback are memory losses and by post-hoc reconstruction. We
        have no other choice:. The first method was used in study 2 and the
        second one in the other studies.  

    \item Estimating \Model{A}{B}{X}. The accuracy of
        \model{A}{B}{X} can be
        estimated in 2 ways.
        \begin{itemize}
            \item Subjective accuracy: In study 3, we compute
                \Model{A}{B}{emotions} by measuring if A describe
                B's emotions in the same way B reports her emotions 
                ($\mathcal{M}(A,B)=\mathcal{M}(B,B)$) 
                
            \item Objective accuracy: In studies 4 and 5, w we compute
                \Model{A}{B}{knowledge} by comparing
                $\mathcal{M}^{\circ}A,B,K)$ to B's actual knowledge as it has
                measured by a test. 

        \end{itemize}

\end{itemize}


\subsubsection{Studies}

The experiments we report here address MM across different tasks, some with
dyads, other with triads. They were conducted along 6 years in two different
institutions by different researchers. They used different independent,
intermediate and dependent variables. Nonetheless, we were able to address a few
questions across these studies. We start with 3 simple hypotheses about
$\mathcal{M}^{\circ}(A,B)$:

\begin{itemize}
    \item $\mathcal{H}_{1}$: $\mathcal{M}^{\circ}(A,B)$ depends upon A's ability or effort
        to model B,
    
    \item $\mathcal{H}_{2}$: $\mathcal{M}^{\circ}(A,B)$ depends upon  B's ability of
        effort to help A to model him/herself 

    \item $\mathcal{H}_{3}$: $\mathcal{M}^{\circ}(A,B)$ depends upon the quality of
        interactions among A and B

\end{itemize}



$\mathcal{H}_{2}$ is also called a second level modeling hypothesis since H
needs to monitor B to see if A understood him/her
$\mathcal{M}(B,(A,\mathcal{M}(A,B)))$

Our main issue is the symmetry questions: what is the relationship
(Fig.~\ref{mm_symmetry}) between \Model{A}{B}{X} and
\Model{B}{A}{X}? A low symmetry would mean that mutual modeling is
mainly an individual attitude/aptitude ($\mathcal{H}_{1}$). A high correlation
might support $\mathcal{H}_{2}$ since there is a low probability that randomly
formed pairs integrate peers with the same level of MM skills.

\begin{figure*}[htb]
\centering

\begin{tikzpicture}[scale=0.5]

\draw(0,0) node[anchor=north] (A) {\scriptsize A};
\draw(10,0) node[anchor=north] (B) {\scriptsize B};
\draw(5,2) node[anchor=north] {\scriptsize \Model{A}{ B}{ X}};
\draw(5,-2) node[anchor=north] {\scriptsize \Model{B}{ A}{ X}};
\draw[dashed, <->] (5,1) -- (5,-1.9) node[midway, sloped, above] {$\Delta_1$};
\draw[->] (A) to[bend left] (B);
\draw[->] (B) to[bend left] (A);

\end{tikzpicture}

\caption{The MM symmetry question $\Delta_1 =  \Delta(\mathcal{M}^{\circ} (A,B,X),
\mathcal{M}^{\circ} (B,A,X))$}

\label{mm_symmetry}
\end{figure*}


With triads, we may compute the accuracy of 6 models:
\Model{A}{B}{X}, \Model{B}{A}{X}, \Model{A}{C}{X}, \Model{C}{A}{X},
\Model{C}{B}{X} and \Model{B}{C}{X}. This enables two
triangle questions (see Fig.~\ref{mm_triangles}):

Do A and B have the same accuracy when modeling C? $\Delta_2 =
\Delta(\M{A}{C}{X}, \M{B}{C}{X})$ If it is the same,
$\mathcal{H}_{2}$ and $\mathcal{H}_{3}$ gain over $\mathcal{H}_{1}$

Does C model more accurately A than B? $\Delta_3= \Delta(\M{C}{A}{X},
\M{C}{B}{X})$ A positive answer would support $\mathcal{H}_{3}$.

\begin{figure}[htb]
\centering
\subcaptionbox{}{ 
    \begin{tikzpicture}[scale=0.5]

    \draw(0,2) node (A) {\scriptsize A};
    \draw(0,-2) node (B) {\scriptsize B};
    \draw(10,0) node (C) {\scriptsize C};

    \draw(5,2) node {\scriptsize \Model{A}{ C}{ X}};
    \draw(5,-2) node {\scriptsize \Model{B}{ C}{ X}};
    \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above] {$\Delta_2$};
    \draw[->] (A) to (C);
    \draw[->] (B) to (C);

    \end{tikzpicture}
}
\subcaptionbox{}{ 
    \begin{tikzpicture}[scale=0.5]

    \draw(0,2) node (A) {\scriptsize A};
    \draw(0,-2) node (B) {\scriptsize B};
    \draw(10,0) node (C) {\scriptsize C};

    \draw(5,2) node {\scriptsize \Model{C}{ A}{ X}};
    \draw(5,-2) node {\scriptsize \Model{C}{ B}{ X}};
    \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above]
    {$\Delta_3$};
    \draw[<-] (A) to (C);
    \draw[<-] (B) to (C);

    \end{tikzpicture}
}
\caption{The MM triangle questions}

\label{mm_triangles}
\end{figure}



In addition, the comparison between $\Delta_2$ and $\Delta_3$ could tell us
whether the accuracy of mutual modelling depends more upon the modeller's effort
or the modellee's behaviour.

\paragraph{The rectangle questions}

We could go further by comparing self- versus other modeling $\Delta_4$ in
Fig.~\ref{mm_rectangle}) as an indication of metacognitive skills. We could also
see if modeling skills depend upon what aspects are being modeled (X or Y),
which would explain vertical differences ($\Delta_5$ in
Fig.~\ref{mm_rectangle}). We do not further develop these questions because we
don’t have the necessary data in our studies.

\begin{figure*}[htb]
\centering

\begin{tikzpicture}[scale=0.5]

    \draw(0,0) node (a) {\scriptsize \Model{A}{ B}{ X}};
    \draw(10,0) node (b) {\scriptsize \Model{A}{ A}{ X}};
    \draw(10,-4) node (c) {\scriptsize \Model{A}{ A}{ Y}};
    \draw(0,-4) node (d) {\scriptsize \Model{A}{ B}{ Y}};
    \draw[<->] (a) -- (b) node[midway, below] {$\Delta_4$};
    \draw[<->] (b) -- (c);
    \draw[<->] (c) -- (d);
    \draw[<->] (d) -- (a) node[midway, sloped, above] {$\Delta_5$};

\end{tikzpicture}

\caption{The rectangle questions}

\label{mm_rectangle}
\end{figure*}


This simple notation does not pretend to provide a mathematical account of
mutual modeling but to be more systematic in describing the following
experiments. 

\subsection{Research agenda}

\subsection{Importance of the work}

\subsection{References}

\bibliographystyle{abbrv}
\bibliography{biblio}


\section{International collaboration}

\end{document}

